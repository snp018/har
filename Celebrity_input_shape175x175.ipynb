!pip install -U --pre tensorflow=="1.*"
!pip install pycocotools

import tensorflow as tf;
import os
import pathlib

import pandas as pd
import cv2
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize
from google.colab.patches import cv2_imshow
import imutils

print(tf.__version__);


import tensorflow as tf
from google.colab import drive
drive.mount('/content/gdrive')

!ls "/content/gdrive/My Drive/dataset/har"

print("Using Tensorflow version")
print(tf.__version__)

basedir='/content/gdrive/My Drive/dataset/har/'

img_width,img_height=175,175

dataset_path = '/content/gdrive/My Drive/project/train'
image_size=(img_width,img_height)
faceArray=[]


#Read images from drive and prepare dataset for training.
def load_emotion_data(faceArray,folder_path,labelList,label):
  images = list(glob.iglob(os.path.join(dataset_path+folder_path, '*.*')))
  detection_model_path = basedir+'data/haarcascades/haarcascade_frontalface_default.xml'
  face_detection = cv2.CascadeClassifier(detection_model_path)
  print(len(images))
  for image in images:
      if not os.path.exists(image):
        raise FileNotFoundError(image)
      print(image)
      img = imread(image)
      #emotion.append(label)
      width, height = img_width, img_height
          #cv2.resize(img,image_size)
      #Detect face from image	  
      faces = face_detection.detectMultiScale(img,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)
      for (x,y,w,h) in faces:
        img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        #cv2_imshow(gray)
        labelList.append(label)
        roi_color = gray[y:y + h, x:x + w]
        face = imutils.resize(roi_color,width=img_width,height=img_height)
        #cv2_imshow(face)
        face = np.asarray(face).reshape(width, height)
        face = cv2.resize(face.astype('uint8'),image_size)
        faceArray.append(face.astype('float32'))
  return faceArray, labelList

def preprocess_input(x, v2=True):
    x = x.astype('float32')
    x = x / 255.0
    if v2:
        x = x - 0.5
        x = x * 2.0
    return x
labelList=[]    
#Load image data from drive
faceArray,labelList = load_emotion_data(faceArray,'/elon',labelList,1)
faceArray,labelList = load_emotion_data(faceArray,'/leo',labelList,2)
faceArray = np.asarray(faceArray)
faces = np.expand_dims(faceArray, -1)
print(labelList)
labelMap = pd.get_dummies(labelList).as_matrix()


faces = preprocess_input(faces)

#Create train and test models

xtrain, xtest,ytrain,ytest = train_test_split(faces, labelMap,test_size=0.2,shuffle=True)

#print(xtrain,xtest,ytrain,ytest)



from  keras.layers import Activation, Convolution2D, Dropout, Conv2D, Dense
from  keras.layers import AveragePooling2D, BatchNormalization
from  keras.layers import GlobalAveragePooling2D
from  keras.models import Sequential
from  keras.layers import Flatten
from  keras.models import Model
from  keras.layers import Input
from  keras.layers import MaxPooling2D
from  keras.layers import SeparableConv2D
from  keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping
from  keras.callbacks import ReduceLROnPlateau
from  keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from  keras import layers
from  keras.regularizers import l2
import pandas as pd
import cv2
import numpy as np
 
# parameters
batch_size = 32
num_epochs = 40
verbose = 1
num_classes = 2
patience = 50
base_path = basedir+'models/'
#print(base_path)
l2_regularization=0.01
 
# data generator
data_generator = ImageDataGenerator(
                        featurewise_center=False,
                        featurewise_std_normalization=False,
                        rotation_range=10,
                        width_shift_range=0.1,
                        height_shift_range=0.1,
                        zoom_range=.1,
                        horizontal_flip=True)
 


#Build a model for facial feature extraction

def build_model():
    model = Sequential()
    
    model.add(Conv2D(32, kernel_size=(5,5), activation='relu', input_shape=(img_width, img_height, 1), padding='same'))
    model.add(Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))
    model.add(Dropout(0.25))
    
    model.add(Conv2D(64, kernel_size=(5,5), activation='relu', padding='same'))
    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))
    model.add(Dropout(0.25))
    
    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax'))
    
    return model


#Load model
model = build_model()
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()
 
logfile = base_path + '_emotion_training.log'
logger = CSVLogger(logfile, append=False)
early_stop = EarlyStopping('val_loss', patience=patience)
reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)
trained_models_path = base_path + '_emotion_train_model'
model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'
model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)
callbacks = [model_checkpoint, logger, early_stop, reduce_lr]
 
#Train model 
model.fit_generator(data_generator.flow(xtrain, ytrain,batch_size),
                        steps_per_epoch=len(xtrain) / batch_size,
                        epochs=num_epochs, verbose=1, callbacks=callbacks,
                        validation_data=(xtest,ytest))



from keras.preprocessing.image import img_to_array
from keras.models import load_model
import imutils
import cv2
import numpy as np
import sys
from google.colab.patches import cv2_imshow
 
basedir='/content/gdrive/My Drive/dataset/har/' 
 # parameters for loading data and images
detection_model_path = basedir+'data/haarcascades/haarcascade_frontalface_default.xml'
emotion_model_path = basedir+'models/_emotion_train_model.40-1.00.hdf5'

img_path = basedir+'image/test-2.jpg'
print(img_path)
 
# hyper-parameters for bounding boxes shape
# loading models
face_detection = cv2.CascadeClassifier(detection_model_path)
emotion_classifier = load_model(emotion_model_path, compile=False)
#EMOTIONS = ["angry","disgust","scared", "happy", "sad", "surprised","neutral"]
EMOTIONS = ["Leo","Elon"]
  
#reading the frame
orig_frame = cv2.imread(img_path)
orig_frame = imutils.resize(orig_frame,width=512,height=512)
#cv2_imshow(orig_frame)
frame = cv2.imread(img_path,0)
frame = imutils.resize(frame,width=512,height=512)
faces = face_detection.detectMultiScale(frame,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)
 
if len(faces) > 0:
    faces = sorted(faces, reverse=True,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]
    (fX, fY, fW, fH) = faces
    roi = frame[fY:fY + fH, fX:fX + fW]
    cv2_imshow(roi)

    roi = cv2.resize(roi, (175, 175))
    roi = roi.astype("float") / 255.0
    roi = img_to_array(roi)
    roi = np.expand_dims(roi, axis=0)
    preds = emotion_classifier.predict(roi)[0]
    emotion_probability = np.max(preds)
    label = EMOTIONS[preds.argmax()]
    cv2.putText(orig_frame, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
    cv2.rectangle(orig_frame, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)

img_rgb = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB)
cv2_imshow(img_rgb)
cv2.imwrite(basedir+'test_output/'+img_path.split('/')[-1],orig_frame)
if (cv2.waitKey(2000) == ord('q')):
    sys.exit("Thanks")
cv2.destroyAllWindows()